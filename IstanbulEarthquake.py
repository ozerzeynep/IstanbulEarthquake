# -*- coding: utf-8 -*-
"""RahvaMakale.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13RShkzuqss-RA3wGUl2qNDyMo-r5EhMH
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from matplotlib.patches import Patch
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")
import time
import shap
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.feature_selection import mutual_info_regression
from catboost import CatBoostRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
from mrmr import mrmr_regression
from scipy.stats import randint
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.neighbors import KNeighborsRegressor
from boruta import BorutaPy
import geopandas as gpd
import folium
from folium import Choropleth
from folium import Html
from shapely.geometry import Point
import os
from scipy.linalg import pinv
from mpl_toolkits.mplot3d import Axes3D

df = pd.read_csv("deprem_senaryosu.csv", encoding="ISO-8859-9", delimiter=";")

df.head(3)

df.tail(3)

df.info()

df.shape

df.columns

df.describe().T

df.isnull().any()

df["ilce_adi"].value_counts()

df.groupby("ilce_adi").agg({"can_kaybi_sayisi":"sum"})

df.groupby(["ilce_adi", "mahalle_adi"]).agg({"can_kaybi_sayisi":"sum"})

sns.set(style="whitegrid")
palette = sns.color_palette("viridis", len(df['ilce_adi'].unique()))
plt.figure(figsize=(15, 8))
sns.barplot(x="ilce_adi", y="can_kaybi_sayisi", data=df, palette=palette)
plt.xlabel("İlçeler", fontsize=14)
plt.ylabel("Ölüm Oranı", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df.can_kaybi_sayisi, kde = True, color='red')
plt.xlabel("Number of Casualties", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
plt.show()

df["can_kaybi_sayisi"].describe()

numeric_df = df.select_dtypes(include=[float, int])

corr = numeric_df.corr()

print(corr)

plt.figure(figsize=(16, 6))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.6, linecolor='gray')
plt.show()

plt.figure(figsize=(16, 6))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor='gray')
plt.title("Correlation Heat Map", fontsize=16, fontweight='bold')
mask = np.zeros_like(corr)
mask[np.abs(corr) < 0.5] = True
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor='gray', mask=mask)
plt.show()

df_grouped = df.groupby('ilce_adi')[['cok_agir_hasarli_bina_sayisi', 'agir_hasarli_bina_sayisi', 'orta_hasarli_bina_sayisi', 'hafif_hasarli_bina_sayisi']].sum()
df_grouped.plot(kind='bar', stacked=True, figsize=(12, 6))
plt.xlabel('İL')
plt.ylabel('Hasarlı Bina Sayısı')
plt.show()

df.plot(kind='scatter', x='cok_agir_hasarli_bina_sayisi', y='agir_hasarli_bina_sayisi', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

plt.figure(figsize=(17, 6))
sns.scatterplot(x="ilce_adi", y="can_kaybi_sayisi", color= "red", data=df)
plt.xticks(rotation=90, fontsize=10)
plt.xlabel("İlçeler", fontsize=14)
plt.ylabel("Ölüm Oranları", fontsize=14)
plt.show()

plt.figure(figsize=(17,6))
sns.lineplot(x= "ilce_adi", y= "can_kaybi_sayisi", data = df);
plt.xticks(rotation = 90, fontsize=9)
plt.xlabel("İlçeler", fontsize= 13)
plt.ylabel("Ölüm Oranları", fontsize=13)
plt.show()

fig, ax1 = plt.subplots(figsize=(13, 6))
color = 'tab:red'
ax1.set_xlabel('İlçeler', fontsize=10)
ax1.set_ylabel('Ağır Hasarlı Bina Sayısı', fontsize=10, color=color)
sns.barplot(x='ilce_adi', y='cok_agir_hasarli_bina_sayisi', data=df, ax=ax1, color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.tick_params(axis='x', rotation=90)
ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('Ölüm Oranı', fontsize=10, color=color)
sns.barplot(x='ilce_adi', y='can_kaybi_sayisi', data=df, ax=ax2, alpha=0.6, color=color)
ax2.tick_params(axis='y', labelcolor=color)
fig.tight_layout()
red_patch = Patch(color='tab:red', label='Ağır Hasarlı Bina Sayısı')
blue_patch = Patch(color='tab:blue', label='Ölüm Oranı')
plt.legend(handles=[red_patch, blue_patch], loc='upper left', bbox_to_anchor=(1.05, 1))
plt.show()

y = df[["can_kaybi_sayisi"]]
x= df.drop(["can_kaybi_sayisi", "ilce_adi", "mahalle_adi"], axis=1)

x_train,x_test,y_train,y_test= train_test_split(x,y, random_state=11, train_size=0.70)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

start_train_time = time.time()
lr = LinearRegression()
model_lr = lr.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time                   #LINEAR REGRESSION(1)

y_train_pred = lr.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = lr.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("-----------------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")

start_train_time = time.time()
rf = RandomForestRegressor(n_estimators=300, random_state=11)
model2 = rf.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time
                                                                            #RONDOM FOREST REGRESSION(2)
y_train_pred = rf.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = rf.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("--------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")

feature_importances = rf.feature_importances_
features = x.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print(importance_df)

plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()

plt.show()

!pip install shap

explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(x)

importance_df = pd.DataFrame({
    'Feature': x.columns,
    'Importance': np.mean(np.abs(shap_values), axis=0)
})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()

param_grid = {
    'C': [0.1, 1, 10],
    'epsilon': [0.01, 0.1, 0.2],
    'kernel': ['linear', 'rbf']  # 'poly' ve diğer kernel türlerini de deneyebilirsiniz
}

grid_search = GridSearchCV(SVR(), param_grid, cv=5)
start_train_time = time.time()
grid_search.fit(x_train_scaled, y_train)
end_train_time = time.time()                                                              #SVR(3)
training_time = end_train_time - start_train_time

best_svr = grid_search.best_estimator_

y_train_pred = best_svr.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_svr.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("--------------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("---------------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2]
}                                                                                         #XGBREGRESSOR(4)

grid_search = GridSearchCV(XGBRegressor(objective='reg:squarederror'), param_grid, cv=5)
start_train_time = time.time()
grid_search.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

best_xgb = grid_search.best_estimator_

y_train_pred = best_xgb.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_xgb.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("----------------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("---------------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

mi = mutual_info_regression(x, y)
mi_df = pd.DataFrame({
    'Feature': x.columns,
    'Mutual Information': mi
}).sort_values(by='Mutual Information', ascending=False)

top_features = mi_df['Feature'].head(5)
x_selected = x[top_features]                                                          #MUTUAL INFO REGRESSOR(5)
print(f"Top Features Based on Mutual Information: {top_features.tolist()}")

start_train_time = time.time()
lr = LinearRegression()
lr.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

y_train_pred = lr.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = lr.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

param_grid = {'fit_intercept': [True, False]}
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5)
grid_search.fit(x_train_scaled, y_train)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

plt.figure(figsize=(10, 6))
plt.barh(mi_df['Feature'], mi_df['Mutual Information'], color='skyblue')
plt.xlabel('Mutual Information')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()




mi = mutual_info_regression(x, y)
mi_df = pd.DataFrame({
    'Feature': x.columns,
    'Mutual Information': mi
}).sort_values(by='Mutual Information', ascending=False)

top_features = mi_df['Feature'].head(5)
x_selected = x[top_features]
print(f"Top Features Based on Mutual Information: {top_features.tolist()}")

start_train_time = time.time()
rf = RandomForestRegressor(random_state=11)
rf.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

y_train_pred = rf.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = rf.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(x_train_scaled, y_train)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("-----------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

plt.figure(figsize=(10, 6))
plt.barh(mi_df['Feature'], mi_df['Mutual Information'], color='skyblue')
plt.xlabel('Mutual Information')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()

!pip install catboost

param_grid = {
    'iterations': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'depth': [3, 5, 7],
    'l2_leaf_reg': [1, 3, 5]
}

grid_search = GridSearchCV(CatBoostRegressor(silent=True), param_grid, cv=5, n_jobs=-1)
start_train_time = time.time()                                                                #CatBoostRegressor(6) (GridSearchCV)
grid_search.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

best_catboost = grid_search.best_estimator_

y_train_pred = best_catboost.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_catboost.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("---------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("--------------------------------------------")
print(f"Best parameters: {grid_search.best_params_}")

param_distributions = {
    'iterations': [100, 200, 300],
    'learning_rate': uniform(0.01, 0.2),
    'depth': [3, 5, 7],
    'l2_leaf_reg': [1, 3, 5]
}

random_search = RandomizedSearchCV(CatBoostRegressor(silent=True), param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=11)
start_train_time = time.time()
random_search.fit(x_train_scaled, y_train)
end_train_time = time.time()                                                                          #CatBoostRegressor(6) (RandomizedSearchCV)
training_time = end_train_time - start_train_time

best_catboost = random_search.best_estimator_

y_train_pred = best_catboost.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_catboost.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("--------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("---------------------------------------------")
print(f"Best parameters: {random_search.best_params_}")

!pip install mrmr_selection

selected_features = mrmr_regression(X=x, y=y, K=5)
x_selected = x[selected_features]

plt.figure(figsize=(10, 6))
plt.barh(selected_features, x[selected_features].mean(), color='skyblue')
plt.xlabel('Average Value')
plt.ylabel('Feature')                                                                       #MRMR(CATBOOSTREGRESSOR)(7)
plt.gca().invert_yaxis()
plt.show()

param_distributions = {
    'iterations': [100, 200, 300],
    'learning_rate': uniform(0.01, 0.2),
    'depth': [3, 5, 7],
    'l2_leaf_reg': [1, 3, 5]
}

random_search = RandomizedSearchCV(CatBoostRegressor(silent=True), param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=11)
start_train_time = time.time()
random_search.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

best_catboost = random_search.best_estimator_

y_train_pred = best_catboost.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_catboost.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("----------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("-----------------------------------------")
print(f"Selected Features: {selected_features}")
print(f"Best parameters: {random_search.best_params_}")

selected_features = mrmr_regression(X=x, y=y, K=5)
x_selected = x[selected_features]

plt.figure(figsize=(10, 6))
plt.barh(selected_features, x[selected_features].mean(), color='skyblue')
plt.xlabel('Average Value')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()                                                                #MRMR(DECISIONTREEREGRESSOR)(7)

param_distributions = {
    'max_depth': randint(3, 10),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(DecisionTreeRegressor(random_state=11), param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=11)
start_train_time = time.time()
random_search.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

best_tree = random_search.best_estimator_

y_train_pred = best_tree.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_tree.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("----------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("---------------------------------------")
print(f"Selected Features: {selected_features}")
print(f"Best parameters: {random_search.best_params_}")

selected_features = mrmr_regression(X=x, y=y, K=5)
x_selected = x[selected_features]

plt.figure(figsize=(10, 6))
plt.barh(selected_features, x[selected_features].mean(), color='skyblue')             #MRMR(ExtraTreesRegressor)(7)
plt.xlabel('Average Value')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()

x_train, x_test, y_train, y_test = train_test_split(x_selected, y, random_state=11, train_size=0.70)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

param_distributions = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 20),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 1.0)
}

random_search = RandomizedSearchCV(ExtraTreesRegressor(random_state=11), param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=11)
start_train_time = time.time()
random_search.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

best_extra_trees = random_search.best_estimator_

y_train_pred = best_extra_trees.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_extra_trees.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("--------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("----------------------------------------")
print(f"Selected Features: {selected_features}")
print(f"Best parameters: {random_search.best_params_}")

selected_features = mrmr_regression(X=x, y=y, K=5)
x_selected = x[selected_features]
                                                                                               #MRMR(KNeighborsRegressor)(7)
plt.figure(figsize=(10, 6))
plt.barh(selected_features, x[selected_features].mean(), color='skyblue')
plt.xlabel('Average Value')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()

param_distributions = {
    'n_neighbors': randint(1, 50),
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

random_search = RandomizedSearchCV(KNeighborsRegressor(), param_distributions, n_iter=50, cv=5, n_jobs=-1, random_state=11)
start_train_time = time.time()
random_search.fit(x_train_scaled, y_train)
end_train_time = time.time()
training_time = end_train_time - start_train_time

best_knn = random_search.best_estimator_

y_train_pred = best_knn.predict(x_train_scaled)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

start_test_time = time.time()
y_test_pred = best_knn.predict(x_test_scaled)
end_test_time = time.time()
testing_time = end_test_time - start_test_time

test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Time: {training_time} seconds")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")
print(f"Training R^2 Score: {train_r2}")
print("--------------------------------------------")
print(f"Testing Time: {testing_time} seconds")
print(f"Testing MSE: {test_mse}")
print(f"Testing MAE: {test_mae}")
print(f"Testing R^2 Score: {test_r2}")
print("--------------------------------------------")
print(f"Selected Features: {selected_features}")
print(f"Best parameters: {random_search.best_params_}")

!pip install boruta

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11, train_size=0.70)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

rf = RandomForestRegressor(n_estimators=100, random_state=11)
boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=11, verbose=2)
boruta_selector.fit(x_train_scaled, y_train.values.ravel())

selected_features = x.columns[boruta_selector.support_]
x_train_selected = x_train[selected_features]
x_test_selected = x_test[selected_features]

explainer = shap.TreeExplainer(rf, feature_perturbation='interventional')
shap_values = explainer.shap_values(x_train_selected, check_additivity=False)

importance_df = pd.DataFrame({
    'Feature': selected_features,
    'Importance': np.mean(np.abs(shap_values), axis=0)
})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance using SHAP and Boruta')
plt.gca().invert_yaxis()
plt.show()

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11, train_size=0.70)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

rf = RandomForestRegressor(n_estimators=100, random_state=11)
boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=11, verbose=2)
boruta_selector.fit(x_train_scaled, y_train.values.ravel())

selected_features = x.columns[boruta_selector.support_]
x_train_selected = x_train[selected_features]
x_test_selected = x_test[selected_features]

rf_selected = RandomForestRegressor(n_estimators=100, random_state=11)
rf_selected.fit(x_train_selected, y_train)

importance_df = pd.DataFrame({
    'Feature': selected_features,
    'Importance': rf_selected.feature_importances_
})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11, train_size=0.70)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

rf = RandomForestRegressor(n_estimators=100, random_state=11)
boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=11, verbose=2)
boruta_selector.fit(x_train_scaled, y_train.values.ravel())

selected_features = x.columns[boruta_selector.support_]

plt.figure(figsize=(10, 6))
plt.barh(selected_features, np.ones_like(selected_features), color='skyblue')
plt.xlabel('Presence')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()

df['toplam_hasarli_bina'] = df['cok_agir_hasarli_bina_sayisi'] + df['agir_hasarli_bina_sayisi'] + df['orta_hasarli_bina_sayisi'] + df['hafif_hasarli_bina_sayisi']
df['toplam_yarali'] = df['agir_yarali_sayisi'] + df['hafif_yarali_sayisi']
df['toplam_boru_hasari'] = df['dogalgaz_boru_hasari'] + df['icme_suyu_boru_hasari'] + df['atik_su_boru_hasari']

plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
sns.histplot(df['toplam_hasarli_bina'], kde=True, color='blue')
plt.xlabel("Total Damaged Building", fontsize= 16)
plt.title('Total Damaged Building Distribution')

plt.subplot(1, 3, 2)
sns.histplot(df['toplam_yarali'], kde=True, color='green')
plt.xlabel("Total Injured", fontsize= 16)
plt.title('Total Injured Distribution')

plt.subplot(1, 3, 3)
sns.histplot(df['toplam_boru_hasari'], kde=True, color='red')
plt.xlabel("Total Pipe Damage", fontsize= 16)
plt.title('Total Pipe Damage Distribution')

plt.tight_layout()
plt.show()

df_grouped = df.groupby('ilce_adi').agg({
    'toplam_hasarli_bina': 'sum',
    'toplam_yarali': 'sum',
    'toplam_boru_hasari': 'sum'
}).reset_index()

plt.figure(figsize=(18, 7))

plt.subplot(1, 3, 1)
sns.barplot(x='ilce_adi', y='toplam_hasarli_bina', data=df_grouped, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel("District", fontsize=16)
plt.ylabel("Total Damaged Building", fontsize=16)
plt.title('Total Damaged Buildings by Districts', fontsize=18)

plt.subplot(1, 3, 2)
sns.barplot(x='ilce_adi', y='toplam_yarali', data=df_grouped, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel("District", fontsize=16)
plt.ylabel("Total Injured", fontsize=16)
plt.title('Total Injured by District', fontsize=18)

plt.subplot(1, 3, 3)
sns.barplot(x='ilce_adi', y='toplam_boru_hasari', data=df_grouped, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel("District", fontsize=16)
plt.ylabel("Total Pipe Damage", fontsize=16)
plt.title('Total Pipe Damage by Districts', fontsize=18)

plt.tight_layout()
plt.show()

#Creates and displays three bar charts showing total damaged buildings, total injured and total pipe damage by district

json_url = 'https://raw.githubusercontent.com/alicangnll/ibb-istanbul-depremi-tahmini-2021/main/ilce.json'
json_data = pd.read_json(json_url)

print(json_data.head())

gdf = gpd.GeoDataFrame(
    json_data,
    geometry=json_data.apply(lambda row: Point(row['lon'], row['lat']), axis=1),
    crs="EPSG:4326"
)

print(gdf.head())

ib_data = pd.read_csv("deprem_senaryosu.csv", encoding="ISO-8859-9", delimiter=";")
ib_data['risk_level'] = ib_data['can_kaybi_sayisi']

gdf = gdf.merge(ib_data[['ilce_adi', 'risk_level']], left_on='ilce_adi', right_on='ilce_adi', how='left')

m = folium.Map(location=[41.0082, 28.9784], zoom_start=11)

Choropleth(
    geo_data=gdf,
    data=gdf,
    columns=['ilce_adi', 'risk_level'],
    key_on='feature.properties.ilce_adi',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Risk Seviyesi'
).add_to(m)

m.save('/content/istanbul_risk_haritasi2.html')

html = Html("""<iframe src="/content/istanbul_risk_haritasi2.html" width="800" height="600"></iframe>""", script=True)

m.add_child(html)

m

def hardlim(x):
    return np.where(x >= 0, 1, 0)

def tribas(x):
    return np.maximum(1 - np.abs(x), 0)

def radbas(x):
    return np.exp(-np.power(x, 2))

def CELM(TrainingData_File, TestingData_File, NumberofHiddenNeurons, ActivationFunction, C):
    train_data = np.loadtxt(TrainingData_File)
    T = train_data[:, 0].T
    P = train_data[:, 1:].T
    test_data = np.loadtxt(TestingData_File)
    TV_T = test_data[:, 0].T
    TV_P = test_data[:, 1:].T
    NumberofTrainingData = P.shape[1]
    NumberofTestingData = TV_P.shape[1]
    NumberofInputNeurons = P.shape[0]

    start_time_train = time.time()

    InputWeight = np.random.rand(NumberofHiddenNeurons, NumberofInputNeurons) * 2 - 1
    BiasofHiddenNeurons = np.random.rand(NumberofHiddenNeurons, 1)

    tempH = np.dot(InputWeight, P) + BiasofHiddenNeurons

    if ActivationFunction in ['sig', 'sigmoid']:
        H = 1 / (1 + np.exp(-tempH))
    elif ActivationFunction == 'sin':
        H = np.sin(tempH)
    elif ActivationFunction == 'hardlim':
        H = hardlim(tempH)
    elif ActivationFunction == 'tribas':
        H = tribas(tempH)
    elif ActivationFunction == 'radbas':
        H = radbas(tempH)

    if C == 10 ** 100:
        OutputWeight = np.dot(pinv(H.T), T.T)
    else:
        OutputWeight = np.linalg.solve(np.eye(H.shape[0]) / C + np.dot(H, H.T), np.dot(H, T.T))

    end_time_train = time.time()
    TrainingTime = end_time_train - start_time_train

    Y = np.dot(H.T, OutputWeight).T
    TrainingRMSE = np.sqrt(np.mean(np.square(T - Y)))
    TrainingMAE = np.mean(np.abs(T - Y))
    TrainingMSE = mean_squared_error(T, Y)
    TrainingR2 = r2_score(T, Y)

    start_time_test = time.time()
    tempH_test = np.dot(InputWeight, TV_P) + BiasofHiddenNeurons
    if ActivationFunction in ['sig', 'sigmoid']:
        H_test = 1 / (1 + np.exp(-tempH_test))
    elif ActivationFunction == 'sin':
        H_test = np.sin(tempH_test)
    elif ActivationFunction == 'hardlim':
        H_test = hardlim(tempH_test)
    elif ActivationFunction == 'tribas':
        H_test = tribas(tempH_test)
    elif ActivationFunction == 'radbas':
        H_test = radbas(tempH_test)
    TY = np.dot(H_test.T, OutputWeight).T
    end_time_test = time.time()
    TestingTime = end_time_test - start_time_test
    TestingRMSE = np.sqrt(np.mean(np.square(TV_T - TY)))
    TestingMAE = np.mean(np.abs(TV_T - TY))
    TestingMSE = mean_squared_error(TV_T, TY)
    TestingR2 = r2_score(TV_T, TY)

    return TrainingTime, TestingTime, TrainingRMSE, TestingRMSE, TrainingMAE, TestingMAE, TrainingMSE, TestingMSE, TrainingR2, TestingR2

activation_functions = ['sig', 'sin', 'hardlim', 'tribas', 'radbas']
neurons_list = [100, 200, 500, 1000, 2000, 5000]
c_list = [10, 100, 1000, 10000, 100000, 10**10]

results_list = []

for act_func in activation_functions:
    for neurons in neurons_list:
        for c in c_list:
            train_data = np.column_stack((y_train, x_train))
            test_data = np.column_stack((y_test, x_test))

            np.savetxt('temp_train.txt', train_data)
            np.savetxt('temp_test.txt', test_data)

            TrainingTime, TestingTime, TrainingRMSE, TestingRMSE, TrainingMAE, TestingMAE, TrainingMSE, TestingMSE, TrainingR2, TestingR2 = CELM(
                'temp_train.txt',
                'temp_test.txt',
                NumberofHiddenNeurons=neurons,
                ActivationFunction=act_func,
                C=c
            )

            results_list.append({
                'Activation': act_func,
                'Neurons': neurons,
                'C': c,
                'Train Time (s)': TrainingTime,
                'Test Time (s)': TestingTime,
                'Train RMSE': TrainingRMSE,
                'Test RMSE': TestingRMSE,
                'Train MAE': TrainingMAE,
                'Test MAE': TestingMAE,
                'Train MSE': TrainingMSE,
                'Test MSE': TestingMSE,
                'Train R²': TrainingR2,
                'Test R²': TestingR2
            })

            os.remove('temp_train.txt')
            os.remove('temp_test.txt')

results = pd.DataFrame(results_list)

print(results.to_string(index=False))

best_rmse = results['Test RMSE'].min()
best_mae = results['Test MAE'].min()
best_mse = results['Test MSE'].min()
best_rkare = results['Test R²'].max()

print("\nEn iyi Test RMSE:")
print(results[results['Test RMSE'] == best_rmse][['Activation', 'Neurons', 'C', 'Test RMSE']].to_string(index=False))

print("\nEn iyi Test MAE:")
print(results[results['Test MAE'] == best_mae][['Activation', 'Neurons', 'C', 'Test MAE']].to_string(index=False))

print("\nEn iyi Test MSE:")
print(results[results['Test MSE'] == best_mse][['Activation', 'Neurons', 'C', 'Test MSE']].to_string(index=False))

print("\nEn iyi Test R²:")
print(results[results['Test R²'] == best_rkare][['Activation', 'Neurons', 'C', 'Test R²']].to_string(index=False))

results.to_csv('results.csv', index=False)

def hardlim(x):
    return np.where(x >= 0, 1, 0)

def tribas(x):
    return np.maximum(1 - np.abs(x), 0)

def radbas(x):
    return np.exp(-np.power(x, 2))

def CELM(TrainingData_File, TestingData_File, NumberofHiddenNeurons, ActivationFunction, C):
    train_data = np.loadtxt(TrainingData_File)
    T = train_data[:, 0].T
    P = train_data[:, 1:].T
    test_data = np.loadtxt(TestingData_File)
    TV_T = test_data[:, 0].T
    TV_P = test_data[:, 1:].T
    NumberofTrainingData = P.shape[1]
    NumberofTestingData = TV_P.shape[1]
    NumberofInputNeurons = P.shape[0]

    start_time_train = time.time()

    InputWeight = np.random.rand(NumberofHiddenNeurons, NumberofInputNeurons) * 2 - 1
    BiasofHiddenNeurons = np.random.rand(NumberofHiddenNeurons, 1)

    tempH = np.dot(InputWeight, P) + BiasofHiddenNeurons

    if ActivationFunction in ['sig', 'sigmoid']:
        H = 1 / (1 + np.exp(-tempH))
    elif ActivationFunction == 'sin':
        H = np.sin(tempH)
    elif ActivationFunction == 'hardlim':
        H = hardlim(tempH)
    elif ActivationFunction == 'tribas':
        H = tribas(tempH)
    elif ActivationFunction == 'radbas':
        H = radbas(tempH)

    if C == 10 ** 100:
        OutputWeight = np.dot(pinv(H.T), T.T)
    else:
        OutputWeight = np.linalg.solve(np.eye(H.shape[0]) / C + np.dot(H, H.T), np.dot(H, T.T))

    end_time_train = time.time()
    TrainingTime = end_time_train - start_time_train

    Y = np.dot(H.T, OutputWeight).T
    TrainingRMSE = np.sqrt(np.mean(np.square(T - Y)))
    TrainingMAE = np.mean(np.abs(T - Y))
    TrainingMSE = np.mean(np.square(T - Y))
    TrainingR2 = r2_score(T, Y)

    start_time_test = time.time()
    tempH_test = np.dot(InputWeight, TV_P) + BiasofHiddenNeurons
    if ActivationFunction in ['sig', 'sigmoid']:
        H_test = 1 / (1 + np.exp(-tempH_test))
    elif ActivationFunction == 'sin':
        H_test = np.sin(tempH_test)
    elif ActivationFunction == 'hardlim':
        H_test = hardlim(tempH_test)
    elif ActivationFunction == 'tribas':
        H_test = tribas(tempH_test)
    elif ActivationFunction == 'radbas':
        H_test = radbas(tempH_test)
    TY = np.dot(H_test.T, OutputWeight).T
    end_time_test = time.time()
    TestingTime = end_time_test - start_time_test
    TestingRMSE = np.sqrt(np.mean(np.square(TV_T - TY)))
    TestingMAE = np.mean(np.abs(TV_T - TY))
    TestingMSE = np.mean(np.square(TV_T - TY))
    TestingR2 = r2_score(TV_T, TY)

    return TrainingTime, TestingTime, TrainingRMSE, TestingRMSE, TrainingMAE, TestingMAE, TrainingMSE, TestingMSE, TrainingR2, TestingR2


X = df.drop(['ilce_adi', 'mahalle_adi', 'can_kaybi_sayisi'], axis=1)
y = df['can_kaybi_sayisi']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

activation_functions = ['sig', 'sin', 'hardlim', 'tribas', 'radbas']
neurons_list = [100, 200, 500, 1000, 2000, 5000]
c_list = [10, 100, 1000, 10000, 100000, 10**10]

results_list = []

for act_func in activation_functions:
    for neurons in neurons_list:
        for c in c_list:
            train_data = np.column_stack((y_train, X_train))
            test_data = np.column_stack((y_test, X_test))

            np.savetxt('temp_train.txt', train_data)
            np.savetxt('temp_test.txt', test_data)

            TrainingTime, TestingTime, TrainingRMSE, TestingRMSE, TrainingMAE, TestingMAE, TrainingMSE, TestingMSE, TrainingR2, TestingR2 = CELM(
                'temp_train.txt',
                'temp_test.txt',
                NumberofHiddenNeurons=neurons,
                ActivationFunction=act_func,
                C=c
            )

            results_list.append({
                'Activation': act_func,
                'Neurons': neurons,
                'C': c,
                'Train Time': TrainingTime,
                'Test Time': TestingTime,
                'Train RMSE': TrainingRMSE,
                'Test RMSE': TestingRMSE,
                'Train MAE': TrainingMAE,
                'Test MAE': TestingMAE,
                'Train MSE': TrainingMSE,
                'Test MSE': TestingMSE,
                'Train R²': TrainingR2,
                'Test R²': TestingR2
            })

            os.remove('temp_train.txt')
            os.remove('temp_test.txt')

results = pd.DataFrame(results_list)

best_rmse = results['Test RMSE'].min()
best_mae = results['Test MAE'].min()
best_mse = results['Test MSE'].min()
best_rkare = results['Test R²'].max()

best_rmse_row = results[results['Test RMSE'] == best_rmse]
best_mae_row = results[results['Test MAE'] == best_mae]
best_mse_row = results[results['Test MSE'] == best_mse]
best_r2_row = results[results['Test R²'] == best_rkare]

best_rmse_activation = best_rmse_row['Activation'].values[0]
best_mae_activation = best_mae_row['Activation'].values[0]
best_mse_activation = best_mse_row['Activation'].values[0]
best_r2_activation = best_r2_row['Activation'].values[0]

best_rmse_coords = (best_rmse_row['Neurons'].values[0], best_rmse_row['C'].values[0])
best_mae_coords = (best_mae_row['Neurons'].values[0], best_mae_row['C'].values[0])
best_mse_coords = (best_mse_row['Neurons'].values[0], best_mse_row['C'].values[0])
best_r2_coords = (best_r2_row['Neurons'].values[0], best_r2_row['C'].values[0])

neurons_list = np.array(neurons_list)
c_list = np.array(c_list)
N, C = np.meshgrid(neurons_list, c_list)

RMSE = np.zeros((len(c_list), len(neurons_list)))
MAE = np.zeros((len(c_list), len(neurons_list)))
MSE = np.zeros((len(c_list), len(neurons_list)))
R2 = np.zeros((len(c_list), len(neurons_list)))

for idx, c in enumerate(c_list):
    for jdx, neurons in enumerate(neurons_list):
        filtered_result = results[(results['Neurons'] == neurons) & (results['C'] == c)]
        if not filtered_result.empty:
            RMSE[idx, jdx] = filtered_result['Test RMSE'].values[0]
            MAE[idx, jdx] = filtered_result['Test MAE'].values[0]
            MSE[idx, jdx] = filtered_result['Test MSE'].values[0]
            R2[idx, jdx] = filtered_result['Test R²'].values[0]
        else:
            RMSE[idx, jdx] = np.nan
            MAE[idx, jdx] = np.nan
            MSE[idx, jdx] = np.nan
            R2[idx, jdx] = np.nan


fig = plt.figure(figsize=(26, 8))
ax1 = fig.add_subplot(131, projection='3d')
surf1 = ax1.plot_surface(N, C, MSE, cmap='viridis', edgecolor='none')
ax1.scatter(best_mse_coords[0], best_mse_coords[1], best_mse, color='r', s=50,
            label=f'Best MSE: {best_mse:.4f} ({best_mse_activation})')
ax1.set_xlabel('Number of Neurons')
ax1.set_ylabel('C (Regularization Parameter)')
ax1.set_zlabel('Test MSE')
ax1.set_title('MSE Surface Plot')
ax1.legend()

ax2 = fig.add_subplot(132, projection='3d')
surf2 = ax2.plot_surface(N, C, MAE, cmap='coolwarm', edgecolor='none', vmin=MAE.min(), vmax=MAE.max())
ax2.scatter(best_mae_coords[0], best_mae_coords[1], best_mae, color='r', s=50,
            label=f'Best MAE: {best_mae:.4f} ({best_mae_activation})')
ax2.set_xlabel('Number of Neurons')
ax2.set_ylabel('C (Regularization Parameter)')
ax2.set_zlabel('Test MAE')
ax2.set_title('MAE Surface Plot')
ax2.legend()

ax3 = fig.add_subplot(133, projection='3d')
surf3 = ax3.plot_surface(N, C, R2, cmap='coolwarm', edgecolor='none', vmin=R2.min(), vmax=R2.max())
ax3.scatter(best_r2_coords[0], best_r2_coords[1], best_rkare, color='b', s=50,
            label=f'Best R²: {best_rkare:.4f} ({best_r2_activation})')
ax3.set_xlabel('Number of Neurons')
ax3.set_ylabel('C (Regularization Parameter)')
ax3.set_zlabel('Test R²')
ax3.set_title('R² Surface Plot')
ax3.legend()

plt.tight_layout()
plt.show()
