# -*- coding: utf-8 -*-
"""DepremIstanbul.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cj9iENDl9xCKZhzG9N6_ZBToLDoIXz4O
"""

import numpy as np                                #Required Libraries Added
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from matplotlib.patches import Patch
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("deprem.csv", encoding="ISO-8859-9", delimiter=";")  #File Included in Page

df.head(3)  #Shows the first 3 rows of the data set

df.tail(3)   #Shows the last 3 rows of the data set

df.info()   #Shows general information of the data set

df.shape  #Returns the dimensions of the data set (number of rows and columns)

df.columns  #Lists the column names in the data set

df.describe().T  #Shows statistical summary of numeric columns by transposing

df.isnull().any()  #Checks which columns have empty values

df["ilce_adi"].value_counts()   #Shows the frequencies of the values in the 'ilce_adi' column

df.groupby("ilce_adi").agg({"can_kaybi_sayisi":"sum"})  #Calculates the total number of casualties for each district

df.groupby(["ilce_adi", "mahalle_adi"]).agg({"can_kaybi_sayisi":"sum"})  #Calculates the total number of casualties for each district and neighborhood

sns.set(style="whitegrid")
palette = sns.color_palette("viridis", len(df['ilce_adi'].unique()))
plt.figure(figsize=(15, 8))
sns.barplot(x="ilce_adi", y="can_kaybi_sayisi", data=df, palette=palette)                         #Creating a bar chart visualizing mortality rates by district
plt.title("Mortality Rates by Districts Using Bar Chart", fontsize=16, fontweight='bold')
plt.xlabel("District", fontsize=14)
plt.ylabel("Mortality Rate", fontsize=14)
plt.xticks(rotation=90, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df.can_kaybi_sayisi, kde = True, color='red')
plt.title("Distribution of the Number of Casualties by Histogram Graph", fontsize=16, fontweight='bold')
plt.xlabel("Number of Casualties", fontsize=14)                                 #Creates and displays a histogram chart visualizing the distribution of the number of casualties
plt.ylabel("Frequency", fontsize=14)
plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
plt.show()

df["can_kaybi_sayisi"].describe()  #Gives a statistical summary of the 'can_kaybi_sayisi' column

numeric_df = df.select_dtypes(include=[float, int])  #Selects numeric (float and int) columns in the data set

corr = numeric_df.corr()  #Calculates the correlation matrix between numeric columns

print(corr)  #Prints the correlation matrix on the screen

plt.figure(figsize=(16, 6))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor='gray')   #Generates and displays heat map visualizing correlations between numerical columns
plt.title("Correlation Heat Map", fontsize=16, fontweight='bold')
plt.show()

plt.figure(figsize=(16, 6))       #Creates and displays the heatmap hiding correlation values less than 0.5
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor='gray')
plt.title("Correlation Heat Map", fontsize=16, fontweight='bold')
mask = np.zeros_like(corr)
mask[np.abs(corr) < 0.5] = True
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor='gray', mask=mask)

plt.show()

fig, ax1 = plt.subplots(figsize=(13, 6))

color = 'tab:red'
ax1.set_xlabel('District Name', fontsize=10)
ax1.set_ylabel('Number of Heavily Damaged Buildings', fontsize=10, color=color)
sns.barplot(x='ilce_adi', y='agir_hasarli_bina_sayisi', data=df, ax=ax1, color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.tick_params(axis='x', rotation=90)
                                              #Constructs and displays a biaxial bar graph showing the number of severely damaged buildings and mortality rate by districts
ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('Mortality Rate', fontsize=10, color=color)
sns.barplot(x='ilce_adi', y='can_kaybi_sayisi', data=df, ax=ax2, alpha=0.6, color=color)
ax2.tick_params(axis='y', labelcolor=color)

plt.title("Bar Chart of Severely Damaged Buildings and Mortality Rate by District", fontsize=14, fontweight='bold')
fig.tight_layout()

red_patch = Patch(color='tab:red', label='Number of Heavily Damaged Buildings')
blue_patch = Patch(color='tab:blue', label='Mortality Rate')
plt.legend(handles=[red_patch, blue_patch], loc='upper left', bbox_to_anchor=(1.05, 1))

plt.show()

plt.figure(figsize=(17, 6))
sns.scatterplot(x="ilce_adi", y="can_kaybi_sayisi", color= "red", data=df)
plt.title("Scatter Plot of Mortality Rates by District", fontsize=16, fontweight= 'bold')
plt.xticks(rotation=90, fontsize=10)            #Creates and displays a scatter graph showing mortality rates by districts
plt.xlabel("Districts", fontsize=14)
plt.ylabel("Mortality Rates", fontsize=14)
plt.show()

plt.figure(figsize=(17,6))
sns.lineplot(x= "ilce_adi", y= "can_kaybi_sayisi", data = df);
plt.title("Line Plot of Mortality Rates by District", fontsize= 17, fontweight= 'bold')
plt.xticks(rotation = 90, fontsize=9)   #Creates and displays a line graph showing mortality rates by districts
plt.xlabel("Districts", fontsize= 13)
plt.ylabel("Mortality Rates", fontsize=13)
plt.show()

y = df[["can_kaybi_sayisi"]]      #Target variable for the model's properties and removes some unnecessary columns
x= df.drop(["can_kaybi_sayisi", "mahalle_koy_uavt", "ilce_adi", "mahalle_adi"], axis=1)

x_train,x_test,y_train,y_test= train_test_split(x,y, random_state=11, train_size=0.70)  #Splits features and target variables into 70% training and 30% testing

scaler = StandardScaler()      #Standardizes and scales training data / Transforms test data with the same scaling
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

rf = RandomForestRegressor(n_estimators=300, random_state=11)  #Creates a 300-tree RandomForestRegressor model and uses a fixed randomness value to control randomness

model2 = rf.fit(x_train_scaled, y_train)    #Trains the RandomForestRegressor model with scaled training data

model2.score(x_test_scaled, y_test)   #Calculates the R² (determination) score of the model on test data and evaluates how well the model performs

y_pred = rf.predict(x_test_scaled)  #Enables the model to make predictions by making predictions on test data

mse = mean_squared_error(y_test, y_pred)  #Calculate the mean squared error (MSE) between actual and predicted values
r2 = r2_score(y_test, y_pred)  #Calculates the R² (determination) score of the model

print(f"RandomForestRegressor MSE: {mse}")  #Prints the mean squared error (MSE) value on the screen
print(f"RandomForestRegressor R^2 Score: {r2}")  #Prints the R² score on the screen

feature_importances = rf.feature_importances_
features = x.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
print(importance_df)

#Calculates the features of the model in order of importance and shows the most important features in order of highest to lowest importance

!pip install shap
import shap

# Define the target variable and features
target = 'can_kaybi_sayisi'
features = df.columns.difference([target, 'ilce_adi', 'mahalle_adi', 'mahalle_koy_uavt'])

X = df[features]
y = df[target]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Use SHAP to explain the model's predictions
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# Plot the feature importance
shap.summary_plot(shap_values, X_test)

lr = LinearRegression()  #Builds a simple linear regression model

model = lr.fit(x_train_scaled, y_train)  #Trains the LinearRegression model with scaled training data

model.score(x_test_scaled,y_test)   #Calculate the R² (determination) score of the model on test data and evaluate the performance of the model

model.score(x_train_scaled, y_train)  #Calculates the R² (stability) score of the model on training data, evaluates the performance of the model on training data and examines the case of overlearning

y_pred = lr.predict(x_test_scaled)  #Produces model predictions by making predictions on test data

mse = mean_squared_error(y_test, y_pred)  #Calculate the mean squared error (MSE) between actual and predicted values
r2 = r2_score(y_test, y_pred)  #Calculates the R² (determination) score of the model

print(f"LinearRegression MSE: {mse}")  #Prints the mean squared error (MSE) value on the screen
print(f"LinearRegression R^2 Score: {r2}")  #Prints the R² score on the screen

# Create DataFrame with predicted and actual values
y_pred = model.predict(X_test)
predictions_df = pd.DataFrame({'Gerçek Değerler': y_test.values.flatten(), 'Tahmin Edilen Değerler': y_pred})
print(predictions_df)

# Compare actual and predicted mortality rates
for i in range(len(predictions_df)):
    print(f"Gerçek Değer: {predictions_df['Gerçek Değerler'].iloc[i]}, Tahmin Edilen Değer: {predictions_df['Tahmin Edilen Değerler'].iloc[i]}")

residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Distribution of Model Residual Errors', fontsize=16, fontweight='bold')
plt.xlabel('Residual Errors', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
plt.show()

#This graph shows the distribution and frequency of differences (residual errors) between model predicted values and actual values

df['toplam_hasarli_bina'] = df['cok_agir_hasarli_bina_sayisi'] + df['agir_hasarli_bina_sayisi'] + df['orta_hasarli_bina_sayisi'] + df['hafif_hasarli_bina_sayisi']
df['toplam_yarali'] = df['agir_yarali_sayisi'] + df['hafif_yarali_sayisi']
df['toplam_boru_hasari'] = df['dogalgaz_boru_hasari'] + df['icme_suyu_boru_hasari'] + df['atik_su_boru_hasari']

plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
sns.histplot(df['toplam_hasarli_bina'], kde=True, color='blue')
plt.xlabel("Total Damaged Building", fontsize= 16)
plt.title('Total Damaged Building Distribution')

plt.subplot(1, 3, 2)
sns.histplot(df['toplam_yarali'], kde=True, color='green')
plt.xlabel("Total Injured", fontsize= 16)
plt.title('Total Injured Distribution')

plt.subplot(1, 3, 3)
sns.histplot(df['toplam_boru_hasari'], kde=True, color='red')
plt.xlabel("Total Pipe Damage", fontsize= 16)
plt.title('Total Pipe Damage Distribution')

plt.tight_layout()
plt.show()

#Creates and displays three histogram plots showing the distribution of total damaged buildings, total injured and total pipe damage

df_grouped = df.groupby('ilce_adi').agg({
    'toplam_hasarli_bina': 'sum',
    'toplam_yarali': 'sum',
    'toplam_boru_hasari': 'sum'
}).reset_index()

plt.figure(figsize=(18, 7))

plt.subplot(1, 3, 1)
sns.barplot(x='ilce_adi', y='toplam_hasarli_bina', data=df_grouped, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel("District", fontsize=16)
plt.ylabel("Total Damaged Building", fontsize=16)
plt.title('Total Damaged Buildings by Districts', fontsize=18)

plt.subplot(1, 3, 2)
sns.barplot(x='ilce_adi', y='toplam_yarali', data=df_grouped, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel("District", fontsize=16)
plt.ylabel("Total Injured", fontsize=16)
plt.title('Total Injured by District', fontsize=18)

plt.subplot(1, 3, 3)
sns.barplot(x='ilce_adi', y='toplam_boru_hasari', data=df_grouped, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel("District", fontsize=16)
plt.ylabel("Total Pipe Damage", fontsize=16)
plt.title('Total Pipe Damage by Districts', fontsize=18)

plt.tight_layout()
plt.show()

#Creates and displays three bar charts showing total damaged buildings, total injured and total pipe damage by district

import geopandas as gpd
import folium
from folium import Choropleth

# Geographic data set loading
geo_data_url = 'https://raw.githubusercontent.com/ozanyerli/istanbul-districts-geojson/main/istanbul-districts.json'
geo_data = gpd.read_file(geo_data_url)
# Upload IBB data
ib_data = pd.read_csv("deprem.csv", encoding="ISO-8859-9", delimiter=";")
ib_data['risk_seviyesi'] = ib_data['can_kaybi_sayisi']

# Combining GeoDataFrame with Pandas DataFrame
geo_data = geo_data.merge(ib_data, left_on='name', right_on='ilce_adi')

# Create the map
m = folium.Map(location=[41.0082, 28.9784], zoom_start=11)

# Showing risk levels on a map
Choropleth(
    geo_data=geo_data,
    data=geo_data,
    columns=['ilce_adi', 'risk_seviyesi'],
    key_on='feature.properties.ilce_adi',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Risk Seviyesi'
).add_to(m)

# Show map
m.save('/content/istanbul_risk_haritasi.html')

!ls /content

from folium import Html

# Create HTML with the map
html = Html("""<iframe src="/content/istanbul_risk_haritasi.html" width="800" height="600"></iframe>""", script=True)

# Add HTML to the map
m.add_child(html)

# Display the map (might not render perfectly)
m

!pip install tensorflow

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Create the model
model = Sequential()
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mse'])

# Model training
history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=50, batch_size=32)

# Evaluate model performance
loss, mse = model.evaluate(X_test_scaled, y_test)
print(f"Derin Öğrenme Modeli MSE: {mse}")

# Make predictions
y_pred = model.predict(X_test_scaled)
r2 = r2_score(y_test, y_pred)
print(f"Derin Öğrenme Modeli R^2 Score: {r2}")

# Visualize the importance of features (using SHAP)
explainer = shap.Explainer(model, X_train_scaled)
shap_values = explainer(X_test_scaled)
shap.summary_plot(shap_values, X_test_scaled)

plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Eğitim Kaybı')
plt.plot(history.history['val_loss'], label='Doğrulama Kaybı')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss of Training and Validation')
plt.legend()
plt.grid(True)
plt.show()

# Add predicted values ​​to DataFrame
predictions_df = pd.DataFrame({'Gerçek Değerler': y_test.values.flatten(), 'Tahmin Edilen Değerler': y_pred.flatten()})

# Compare actual and predicted values
print(predictions_df.head())


plt.figure(figsize=(10, 6))
plt.scatter(predictions_df['Gerçek Değerler'], predictions_df['Tahmin Edilen Değerler'], alpha=0.5)
plt.xlabel('Gerçek Değerler')
plt.ylabel('Tahmin Edilen Değerler')
plt.title('Gerçek vs Tahmin Edilen Değerler')
plt.grid(True)
plt.show()

import shap

explainer = shap.Explainer(model, X_train_scaled)
shap_values = explainer(X_test_scaled)

shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns)

